{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29c2bcca",
   "metadata": {},
   "source": [
    "# CMGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd05a7d",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56b1de7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a79aeac",
   "metadata": {},
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "740ced5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc39dbd7",
   "metadata": {},
   "source": [
    "### GPU State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b76ef32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jul 11 20:14:51 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 561.03                 Driver Version: 561.03         CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4070 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   49C    P0             16W /   95W |       0MiB /   8188MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "# In terminal\n",
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6752c0e7",
   "metadata": {},
   "source": [
    "### Conformer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "201ad617",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConformerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads=4, conv_kernel_size=31):\n",
    "        super().__init__()\n",
    "        self.ffn1 = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(dim * 4, dim)\n",
    "        )\n",
    "\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(dim, dim * 2, kernel_size=1),\n",
    "            nn.GLU(dim=1),\n",
    "            nn.Conv1d(dim, dim, kernel_size=conv_kernel_size, padding=conv_kernel_size // 2, groups=dim),\n",
    "            nn.BatchNorm1d(dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv1d(dim, dim, kernel_size=1),\n",
    "        )\n",
    "\n",
    "        self.ffn2 = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(dim * 4, dim)\n",
    "        )\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):  # x: [B, T, dim]\n",
    "        x = x + 0.5 * self.ffn1(x)\n",
    "        attn_out, _ = self.mha(x, x, x)\n",
    "        x = x + attn_out\n",
    "\n",
    "        conv_in = x.transpose(1, 2)  # [B, dim, T]\n",
    "        conv_out = self.conv(conv_in).transpose(1, 2)  # [B, T, dim]\n",
    "        x = x + conv_out\n",
    "\n",
    "        x = x + 0.5 * self.ffn2(x)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edba0406",
   "metadata": {},
   "source": [
    "### Generator Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c1efff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- Dilated DenseNet Block ---\n",
    "class DilatedDenseBlock(nn.Module):\n",
    "    def __init__(self, in_channels, growth_rate=16, num_layers=4, out_channels=None):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.growth_rate = growth_rate\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        channels = in_channels\n",
    "        for i in range(num_layers):\n",
    "            self.layers.append(\n",
    "                nn.Conv2d(channels, growth_rate, kernel_size=3, dilation=2**i, padding=2**i)\n",
    "            )\n",
    "            channels += growth_rate\n",
    "\n",
    "        self.output_conv = nn.Conv2d(channels, out_channels, kernel_size=1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = [x]\n",
    "        for layer in self.layers:\n",
    "            x_cat = torch.cat(features, dim=1)\n",
    "            out = self.relu(layer(x_cat))\n",
    "            features.append(out)\n",
    "        x_cat = torch.cat(features, dim=1)\n",
    "        return self.output_conv(x_cat)\n",
    "\n",
    "# --- CMGAN Generator ---\n",
    "class CMGANGenerator(nn.Module):\n",
    "    def __init__(self, n_fft=512, hop_length=128, conformer_dim=64, num_blocks=4):\n",
    "        super().__init__()\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.register_buffer(\"window\", torch.hann_window(self.n_fft), persistent=False)\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),              # Pre-projection\n",
    "            nn.ReLU(),\n",
    "            DilatedDenseBlock(in_channels=64, out_channels=128),     # Dilated DenseNet\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),           # Refinement\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, conformer_dim, kernel_size=1)             # Output projection\n",
    "        )\n",
    "\n",
    "        # Conformer blocks\n",
    "        self.conformers = nn.Sequential(\n",
    "            *[ConformerBlock(dim=conformer_dim) for _ in range(num_blocks)]\n",
    "        )\n",
    "\n",
    "        # Mask decoder\n",
    "        self.mask_decoder = nn.Sequential(\n",
    "            DilatedDenseBlock(in_channels=conformer_dim, out_channels=conformer_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "            nn.Conv2d(conformer_dim, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 1, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Complex decoder\n",
    "        self.complex_decoder = nn.Sequential(\n",
    "            DilatedDenseBlock(in_channels=conformer_dim, out_channels=conformer_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "            nn.Conv2d(conformer_dim, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 2, kernel_size=1)  # real & imag\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, _, T = x.shape\n",
    "\n",
    "        # STFT\n",
    "        stft = torch.stft(x.squeeze(1), n_fft=self.n_fft, hop_length=self.hop_length, window=self.window, return_complex=True)\n",
    "\n",
    "        mag = stft.abs()      # [B, F, T']\n",
    "        real = stft.real\n",
    "        imag = stft.imag\n",
    "        phase = stft.angle()\n",
    "\n",
    "        # Stack input features [B, 3, F, T']\n",
    "        input_feat = torch.stack([mag, real, imag], dim=1)\n",
    "\n",
    "        # Encode\n",
    "        x_encoded = self.encoder(input_feat)              # [B, C, F, T']\n",
    "        x_pooled = torch.mean(x_encoded, dim=2)           # [B, C, T']\n",
    "        x_seq = x_pooled.permute(0, 2, 1)                 # [B, T', C]\n",
    "\n",
    "        # Conformer blocks\n",
    "        x_seq = self.conformers(x_seq)                    # [B, T', C]\n",
    "\n",
    "        # Reshape for decoding\n",
    "        x = x_seq.permute(0, 2, 1).unsqueeze(2)           # [B, C, 1, T']\n",
    "        x = x.expand(-1, -1, mag.shape[1], -1)            # [B, C, F, T']\n",
    "\n",
    "        # Decoders\n",
    "        mask = self.mask_decoder(x)                       # [B, 1, F, T\"]\n",
    "        complex_pred = self.complex_decoder(x)            # [B, 2, F, T\"]\n",
    "\n",
    "        # Resize to match mag shape\n",
    "        if mask.shape[-1] != mag.shape[-1]:\n",
    "            mask = F.interpolate(mask, size=mag.shape[-1], mode='bilinear', align_corners=False)\n",
    "        if mask.shape[-2] != mag.shape[-2]:\n",
    "            mask = F.interpolate(mask, size=mag.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        if complex_pred.shape[-2:] != mag.shape[-2:]:\n",
    "            complex_pred = F.interpolate(complex_pred, size=mag.shape[-2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Masked magnitude + phase reconstruction\n",
    "        enhanced_mag = mag.unsqueeze(1) * mask            # [B, 1, F, T']\n",
    "        masked_real = enhanced_mag.squeeze(1) * torch.cos(phase)\n",
    "        masked_imag = enhanced_mag.squeeze(1) * torch.sin(phase)\n",
    "        masked_complex = torch.complex(masked_real, masked_imag)\n",
    "\n",
    "        # iSTFT\n",
    "        enhanced_waveform = torch.istft(masked_complex, n_fft=self.n_fft, hop_length=self.hop_length, length=T, window=self.window)\n",
    "\n",
    "        return {\n",
    "            'enhanced_waveform': enhanced_waveform.unsqueeze(1),\n",
    "            'predicted_complex': complex_pred,\n",
    "            'mask': mask\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252a4b07",
   "metadata": {},
   "source": [
    "### Metric discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02956464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MetricDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(2, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),  # → (B, 128, 1, 1)\n",
    "            nn.Flatten(),                  # → (B, 128)\n",
    "            nn.Linear(128, 1),             # → (B, 1)\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, clean_mag, enhanced_mag):\n",
    "        x = torch.stack([clean_mag, enhanced_mag], dim=1)  # → (B, 2, F, T)\n",
    "        return self.net(x)  # → (B, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c86460",
   "metadata": {},
   "source": [
    "### TF Loss (Ltf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2c084c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TFLoss(nn.Module):\n",
    "    def __init__(self, n_fft=512, hop_length=128, alpha=0.7):\n",
    "        super().__init__()\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.alpha = alpha  # α in the paper\n",
    "\n",
    "    def forward(self, enhanced, clean):\n",
    "        # Create Hann window on the same device as input\n",
    "        window = torch.hann_window(self.n_fft).to(enhanced.device)\n",
    "\n",
    "        # STFT with window\n",
    "        est_stft = torch.stft(enhanced.squeeze(1), n_fft=self.n_fft, hop_length=self.hop_length, \n",
    "                              return_complex=True, window=window)\n",
    "        clean_stft = torch.stft(clean.squeeze(1), n_fft=self.n_fft, hop_length=self.hop_length, \n",
    "                                return_complex=True, window=window)\n",
    "\n",
    "        # Magnitude loss (L_Mag)\n",
    "        mag_enh = est_stft.abs()\n",
    "        mag_clean = clean_stft.abs()\n",
    "        l_mag = F.mse_loss(mag_enh, mag_clean)\n",
    "\n",
    "        # Complex loss (L_RI)\n",
    "        l_ri = F.mse_loss(est_stft.real, clean_stft.real) + F.mse_loss(est_stft.imag, clean_stft.imag)\n",
    "\n",
    "        # Final TF loss\n",
    "        l_tf = self.alpha * l_mag + (1 - self.alpha) * l_ri\n",
    "        return l_tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f971cc04",
   "metadata": {},
   "source": [
    "### GAN Loss (Lgan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86dbe336",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, D, clean_mag, enhanced_mag, qpesq_score=None):\n",
    "        pred_fake = D(clean_mag, enhanced_mag)  # → [B, 1]\n",
    "        pred_real = D(clean_mag, clean_mag)     # → [B, 1]\n",
    "\n",
    "        # LGAN: Generator loss (wants D to output 1 for fake)\n",
    "        g_loss = F.mse_loss(pred_fake, torch.ones_like(pred_fake))\n",
    "\n",
    "        # LD: Discriminator loss\n",
    "        real_loss = F.mse_loss(pred_real, torch.ones_like(pred_real))\n",
    "\n",
    "        if qpesq_score is None:\n",
    "            qpesq_score = torch.zeros_like(pred_fake)  # → match shape [B, 1]\n",
    "\n",
    "        fake_loss = F.mse_loss(pred_fake, qpesq_score)\n",
    "        d_loss = real_loss + fake_loss\n",
    "\n",
    "        return g_loss, d_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef4dfcb",
   "metadata": {},
   "source": [
    "### Time Domain Loss (Ltime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e5225d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeDomainLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.L1Loss()\n",
    "\n",
    "    def forward(self, enhanced, clean):\n",
    "        return self.l1(enhanced, clean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adbd84f",
   "metadata": {},
   "source": [
    "CMGAN Loss (Lg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c04b3f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CMGANLoss(nn.Module):\n",
    "    def __init__(self, gamma1=1.0, gamma2=1.0, gamma3=1.0, alpha=0.7, n_fft=512, hop_length=128):\n",
    "        super().__init__()\n",
    "        self.tf_loss = TFLoss(alpha=alpha, n_fft=n_fft, hop_length=hop_length)\n",
    "        self.gan_loss = GANLoss()\n",
    "        self.time_loss = nn.L1Loss()\n",
    "        self.gamma1 = gamma1\n",
    "        self.gamma2 = gamma2\n",
    "        self.gamma3 = gamma3\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "\n",
    "    def forward(self, enhanced, clean, D, qpesq_score=None):\n",
    "        # Time-frequency loss\n",
    "        l_tf = self.tf_loss(enhanced, clean)\n",
    "\n",
    "        # Time-domain loss\n",
    "        l_time = self.time_loss(enhanced, clean)\n",
    "\n",
    "        # GAN loss\n",
    "        window = torch.hann_window(self.n_fft).to(enhanced.device)\n",
    "        est_stft = torch.stft(enhanced.squeeze(1), n_fft=self.n_fft, hop_length=self.hop_length, return_complex=True, window=window)\n",
    "        clean_stft = torch.stft(clean.squeeze(1), n_fft=self.n_fft, hop_length=self.hop_length, return_complex=True, window=window)\n",
    "\n",
    "        est_mag = est_stft.abs()\n",
    "        clean_mag = clean_stft.abs()\n",
    "\n",
    "        g_loss, _ = self.gan_loss(D, clean_mag, est_mag, qpesq_score)\n",
    "\n",
    "        # Final combined generator loss\n",
    "        l_gen = self.gamma1 * l_tf + self.gamma2 * g_loss + self.gamma3 * l_time\n",
    "        return l_gen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462b985c",
   "metadata": {},
   "source": [
    "Discriminator Loss (Ld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5260a893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_discriminator_loss(D, enhanced, clean, qpesq_score=None, n_fft=512, hop_length=128):\n",
    "    # Create Hann window on the same device\n",
    "    window = torch.hann_window(n_fft).to(enhanced.device)\n",
    "\n",
    "    # Compute STFTs\n",
    "    est_stft = torch.stft(enhanced.squeeze(1), n_fft=n_fft, hop_length=hop_length,\n",
    "                          return_complex=True, window=window)\n",
    "    clean_stft = torch.stft(clean.squeeze(1), n_fft=n_fft, hop_length=hop_length,\n",
    "                            return_complex=True, window=window)\n",
    "\n",
    "    # Get magnitude spectrograms\n",
    "    est_mag = est_stft.abs()\n",
    "    clean_mag = clean_stft.abs()\n",
    "\n",
    "    # Discriminator outputs\n",
    "    pred_real = D(clean_mag, clean_mag)      # shape: [B, 1]\n",
    "    pred_fake = D(clean_mag, est_mag)        # shape: [B, 1]\n",
    "\n",
    "    # Targets must match shape [B, 1]\n",
    "    real_target = torch.ones_like(pred_real)\n",
    "    fake_target = qpesq_score if qpesq_score is not None else torch.zeros_like(pred_fake)\n",
    "\n",
    "    # Losses\n",
    "    real_loss = F.mse_loss(pred_real, real_target)\n",
    "    fake_loss = F.mse_loss(pred_fake, fake_target)\n",
    "\n",
    "    # Final loss\n",
    "    d_loss = real_loss + fake_loss\n",
    "    return d_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c3e65eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_loss_fn = CMGANLoss(gamma1=1.0, gamma2=1.0, gamma3=1.0).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa30f113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f32f6660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(generator, discriminator, dataloader, g_loss_fn, g_optimizer, d_optimizer, epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "\n",
    "        for batch in dataloader:\n",
    "            noisy = batch['noisy'].cuda()\n",
    "            clean = batch['clean'].cuda()\n",
    "\n",
    "            # Generator forward\n",
    "            outputs = generator(noisy)\n",
    "            enhanced = outputs['enhanced_waveform']\n",
    "\n",
    "            if epoch % 4 ==0:\n",
    "                # Discriminator update\n",
    "                d_optimizer.zero_grad()\n",
    "                d_loss = compute_discriminator_loss(discriminator, enhanced.detach(), clean)\n",
    "                d_loss.backward()\n",
    "                d_optimizer.step()\n",
    "\n",
    "            # Generator update\n",
    "            g_optimizer.zero_grad()\n",
    "            g_loss = g_loss_fn(enhanced, clean, discriminator)\n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] G Loss: {g_loss.item():.4f} | D Loss: {d_loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67a0554f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import librosa\n",
    "\n",
    "class ChunkedAudioDataset(Dataset):\n",
    "    def __init__(self, noisy_dir, clean_dir, chunk_size=16000, sr=16000, max_files=500):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.sr = sr\n",
    "        self.pairs = []\n",
    "\n",
    "        noisy_files = sorted(Path(noisy_dir).glob(\"*.wav\"))[:max_files]\n",
    "\n",
    "        for file in noisy_files:\n",
    "            filename = file.name\n",
    "            clean_file = Path(clean_dir) / filename\n",
    "            if not clean_file.exists():\n",
    "                continue\n",
    "\n",
    "            duration = librosa.get_duration(path=str(file))\n",
    "            total_samples = int(duration * sr) // chunk_size * chunk_size\n",
    "            num_chunks = total_samples // chunk_size\n",
    "\n",
    "            for i in range(num_chunks):\n",
    "                self.pairs.append((file, clean_file, i * chunk_size))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        noisy_path, clean_path, offset = self.pairs[idx]\n",
    "        noisy, _ = librosa.load(noisy_path, sr=self.sr, offset=offset / self.sr, duration=self.chunk_size / self.sr)\n",
    "        clean, _ = librosa.load(clean_path, sr=self.sr, offset=offset / self.sr, duration=self.chunk_size / self.sr)\n",
    "\n",
    "        noisy = torch.tensor(noisy, dtype=torch.float32).unsqueeze(0)\n",
    "        clean = torch.tensor(clean, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        return {'noisy': noisy, 'clean': clean}\n",
    "\n",
    "dataset = ChunkedAudioDataset(\n",
    "    noisy_dir=r\"E:\\noisy_sound\",\n",
    "    clean_dir=r\"D:\\vs_code_python\\Sperctromorph_GANs\\data\\clean_testset_wav\",\n",
    "    chunk_size=16000\n",
    ")\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "generator = CMGANGenerator().cuda()\n",
    "discriminator = MetricDiscriminator().cuda()\n",
    "g_loss_fn = CMGANLoss(gamma1=1.0, gamma2=1.0, gamma3=1.0).cuda()\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=1e-4)\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=1e-4)\n",
    "\n",
    "#train(generator, discriminator, dataloader, g_loss_fn, g_optimizer, d_optimizer, epochs=50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7676f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save generator\n",
    "#torch.save(generator.state_dict(), \"cmgan_generator.pth\")\n",
    "\n",
    "# Optional: Save discriminator too\n",
    "#torch.save(discriminator.state_dict(), \"cmgan_discriminator.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfeeea1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6076\\2246475223.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  generator.load_state_dict(torch.load(\"cmgan_generator.pth\", map_location='cpu'))  # or 'cuda' if using GPU\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6076\\2246475223.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  discriminator.load_state_dict(torch.load(\"cmgan_discriminator.pth\", map_location='cpu'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.load_state_dict(torch.load(\"cmgan_generator.pth\", map_location='cpu'))  # or 'cuda' if using GPU\n",
    "discriminator.load_state_dict(torch.load(\"cmgan_discriminator.pth\", map_location='cpu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "904e221b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "def enhance_audio(generator, input_path, output_path, sr=16000, device='cuda'):\n",
    "    generator.eval()\n",
    "    input_path = Path(input_path)\n",
    "    output_path = Path(output_path)\n",
    "\n",
    "    # Load and preprocess\n",
    "    waveform, file_sr = torchaudio.load(str(input_path))\n",
    "    if file_sr != sr:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=file_sr, new_freq=sr)\n",
    "        waveform = resampler(waveform)\n",
    "\n",
    "    waveform = waveform.mean(dim=0, keepdim=True)  # convert to mono if stereo\n",
    "    waveform = waveform.unsqueeze(0).to(device)    # [1, 1, T]\n",
    "\n",
    "    # Forward pass through CMGAN\n",
    "    with torch.no_grad():\n",
    "        outputs = generator(waveform)\n",
    "        enhanced = outputs['enhanced_waveform']     # [1, 1, T]\n",
    "        enhanced = enhanced.squeeze(0).cpu()        # [1, T]\n",
    "\n",
    "    # Save the output\n",
    "    torchaudio.save(str(output_path), enhanced, sample_rate=sr)\n",
    "    print(f\"[✓] Enhanced audio saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "025e4212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] Enhanced audio saved to: E:\\final_cleaned.wav\n"
     ]
    }
   ],
   "source": [
    "enhance_audio(generator, r\"E:\\filtered_output_freq_removal.wav\", r\"E:\\final_cleaned.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779da8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "#torch.save(generator.state_dict(), \"cmgan_generator.pth\")\n",
    "#torch.save(discriminator.state_dict(), \"cmgan_discriminator.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3134dd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator output shape: torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "# Dummy clean and enhanced waveforms\n",
    "waveform = torch.randn(2, 1, 16000).cuda()  # (B=2, C=1, T=16000)\n",
    "n_fft = 512\n",
    "hop_length = 128\n",
    "window = torch.hann_window(n_fft).cuda()\n",
    "\n",
    "# STFT\n",
    "clean_stft = torch.stft(waveform.squeeze(1), n_fft=n_fft, hop_length=hop_length,\n",
    "                        return_complex=True, window=window)\n",
    "enhanced_stft = torch.stft(waveform.squeeze(1), n_fft=n_fft, hop_length=hop_length,\n",
    "                           return_complex=True, window=window)\n",
    "\n",
    "clean_mag = clean_stft.abs()\n",
    "enhanced_mag = enhanced_stft.abs()\n",
    "\n",
    "# Pass through discriminator\n",
    "with torch.no_grad():\n",
    "    out = discriminator(clean_mag, enhanced_mag)\n",
    "    print(\"Discriminator output shape:\", out.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d7c7b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
